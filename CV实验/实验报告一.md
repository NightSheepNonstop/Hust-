## 1 训练与测试
---
### 1.1 神经网络架构

- 包括了一个输入层，一个隐藏层和一个输出层
- 神经元数量为2：10：4
- 激活函数选用RELU函数
- 具体架构如图所示：
![[fcnn.png]]

### 1.2 损失

- 我们将训练集和测试集的batch_size都设置为40
- 在每一次mini_batch训练后，损失如图所示：![[loss1.png]]
- 在每一次epoch后，损失如图所示：![[loss3.png]]
- 可以看出大概只需1-2轮训练就可以使模型收敛了

### 1.3 准确率

- epoch数为5
- 在训练集上的准确率为93%，在测试集上的准确率为92%，如图所示：![[acc1.png]]

## 2 尝试一：增加网络层数
---
- 在增加了两层层隐藏层后神经网络结构如图所示：
- 模型精度并无明显变化
- 分析：数据真实模型比较简单，不需要复杂的模型，增加隐藏层的作用不大

![[fcnn1.png]]

## 3  尝试二：修改神经元个数
---
- 尝试分别将隐藏层神经元个数改为2，5，15
### 3.1 神经元个数为二

- 神经元个数为二时，精度有明显下降：![[acc2.png]]
### 3.2 神经元个数为五

- 神经元个数为五时，精度与初始精度相差不大：![[acc3.png]]

### 3.3 神经元个数为十五

- 神经元个数为十五时，精度相差同样不大：![[acc4.png]]

### 3.4 结论

- **在epoch为5的情况下**，神经元个数在**五到十之间**就可以得到比较好的训练效果了。

## 4  尝试三：使用不同的激活函数
---
### 4.1 sigmoid函数

- 在epoch为5的情况下，使用sigmoid函数的准确率并没有明显变化，但是可以从图中看出，模型收敛需要的周期数更多了，说明sigmoid函数的效率在此例中低于relu
![[loss2.png]]

### 4.2 tanh函数

- tanh函数的收敛速度慢于relu，但要快于sigmoid![[loss4.png]]