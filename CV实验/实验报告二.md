## 1 总述
---
### 1.1 神经网络架构

- 神经网络架构模仿的是ResNet-18的架构;
- 激活函数选用ReLU函数；
- 网络中包括五个卷积组，将后四个卷积组命名为`layer1`-`layer4`，每个`layer`包括两个残差块，每个残差块包括两个卷积层，最后通过一个全连接层输出，所以总共为18层:
	- 第1个卷积组只包含1次卷积计算操作，卷积核为$7 \times7$， 步长为2，padding为3;
	- `layer1` 包含两个残差块，卷积核大小都为$3 \times3$,`layer1`步长为1，padding为1，所以`layer1`不会进行降采样;
	- `layer2-layer4`，每个 `layer` 包含 2 个 残差块，卷积核大小都为$3 \times3$,但是第 1 个 残差块 的第 1 个卷积层的步长为2，会进行降采样。在进行`shortcut`连接时，会经过`downsample`层，进行降采样和降维。而第二个卷积层步长为1，不会进行下采样，`shortcut`连接也不会经过`downsample`层

### 1.2 损失

- 共10个epoch，训练集batch大小为64，学习率为0.01
- 截取训练时的部分输出如图所示：![[Pasted image 20231218215327.png]]
- 每个epoch结束后的loss如图所示：![[Pasted image 20231218220343.png]]
- 模型在10个epoch之后，Loss缩小至了0.20056

### 1.3 准确率

- 在训练集上的准确率：![[Pasted image 20231218220630.png]]
- 在测试集上的准确率：![[Pasted image 20231218220736.png]]
- 可以看出在ResNet18架构下，训练集和测试集的精度都达到了百分之九十九以上，其中训练效果最好的数字为'8'；训练效果最差的数字为'2'和'7'，猜测是因为'2'和'7'比较相似，存在混淆的现象

## 2 尝试一：使用不同的激活函数
---
### 2.1 sigmoid函数

- 我们主要关注使用不同激活函数之后，模型损失的收敛速度，如图所示：![[Pasted image 20231218222056.png]]
- 对比使用relu函数的结果，我们可以看出sigmoid函数的性能要差于relu函数

### 2.2 tanh函数

- 收敛速度如图所示：![[Pasted image 20231218222938.png]]
- 对比relu和sigmoid的结果，可以发现tanh函数的性能略逊于relu，优于sigmoid

## 3 尝试二：修改超参数
---
### 3.1 修改学习率

- 将学习率从0.01更改至0.1
- 训练过程中的损失如图所示：![[Pasted image 20231218224149.png]]
- 在训练集上的精确率：![[Pasted image 20231218224235.png]]
- 在测试集上的精确率：![[Pasted image 20231218224305.png]]
- 可以看出在学习率为0.1的情况下，模型的精度下降了不少，学习率设置为0.01是一个更优的选择

### 3.2 修改batch_size

- 将训练集的batch_size从64修改为1000
- 训练过程中的损失如图所示：![[Pasted image 20231218225419.png]]
- 在训练集上的精确率：![[Pasted image 20231218225457.png]]
- 在测试集上的准确率：![[Pasted image 20231218225526.png]]
- 可以看出，无论是模型收敛速度还是精确率，batch_size修改后的模型效果都优于修改前的。batch_size设置为1000是一个更合理的超参数